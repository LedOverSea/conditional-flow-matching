其他工作:将算法目标形式化表达, 添加图片,修改格式

# 12.7
复现论文结果, 已经开源的是类胚体计算演化轨迹的例子, 以下是未开源的, 需要手动实现:

对于类胚体（EB）数据集，采用与原始研究一致的预处理结果，该数据集包含数据的前 100 个主成分（principal components）。在表 4 的实验中，我们先将数据截断至前 5 个维度，随后参照 Tong 等人（2020 年）的方法对每个维度进行白化（whitening）处理，再执行插值操作。该类胚体数据集包含 30 天内采集的 5 个时间点数据，通过依次留一（leave out）时间点 1、2、3 的方式训练独立模型：测试阶段，将所有观测数据点 Xₜ₋₁映射（push forward）至目标时间点 t，随后计算预测分布与真实分布之间的 1 - 瓦瑟斯坦距离（1-Wasserstein distance）作为评估指标。

模型的评估方法采用皮尔逊相关系数。对于Multiome 数据集，对每个样本计算真实基因表达值与预测基因表达值之间的皮尔逊相关系数；针对CITEseq 数据集，对每个样本计算真实表面蛋白水平与预测表面蛋白水平之间的皮尔逊相关系数。模型整体得分为所有样本相关系数的平均值。

# 12.8
cfm通用算法框架:

采样获得批次x0, x1 (bs, dim)

(可选)用ot算法计算最优配对的样本对,重新赋值给x0, x1 (bs, dim)

采样获得位置xt,向量场ut

然后用深度学习模型训练向量场的预测值vt, 原文给mlp,可尝试用dnn, gnn

sf2m模型不收敛, 尝试解决方法: 学习率衰减, 梯度裁剪, 效果甚微

# 12.9
runner运行环境冲突

pytorch-lightning 1.8.3.post2 depends on torch>=1.9.*

    torchmetrics 0.11.0 depends on torch>=1.8.1

    torchvision 0.24.1 depends on torch==2.9.1

原因: kaggle默认较新版本的python,torch. 原文未给出python版本,所以这个错误非常隐蔽.

python版本较低才能使用,python==3.9可用.

切换python版本的具体方法见kaggle中一个python-version的笔记本

仍然报错,kaggle不支持一个交互式界面,已力竭

# 12.10
本地创建虚拟环境cfm-runner,需要这些原文未提及的依赖

python=3.9

pip=24.0

numpy=1.26.4

Deprecated

tasklogger>=1.2

rich==12.6.0

torch默认下载cpu版本, 先手动下载gpu版本

# 12.11
复现eb类胚体数据结果

cd runner

python .\src\train.py datamodule=custom_dist

embryoid_anndata_small_v2.h5ad是用ebdata_v3.h5ad数据集重命名的, datamodule对其进行了分割,所以最后图像只显示部分测试集

# 12.12
运行数据模块, 编写配置文件train_1212.yaml, 脚本1212.py, 运行方法

```powershell
python .\src\1212.py
```

配置文件: 把默认的datamodule改成custom_dist

## 脚本功能
总体功能: 自动读取配置文件, 然后输出配置信息(json格式), 并且输出数据集

实现细节: datamodule是实例化的TrajectoryNetDistributionTrajectoryDataModule

然后查看这个类中的主要属性: data, dim, label, system, data.shape

data是(18203, 2)的矩阵, 由于配置文件custom_dist.yaml中embedding_name: "phate", 即是phate降维的数据(可选的还有pca等, 尚未使用)

dim: 2  因为phate降维到2维

label: ['Day 00-03' 'Day 00-03' 'Day 00-03' ... 'Day 24-27' 'Day 24-27'
 'Day 24-27']   时间标签, (18203, )的向量, 共5个时间点

system: D:\desktop\code\conditional-flow-matching\runner/data//embryoid_anndata_small_v2.h5ad   数据文件目录

## datamodule中处理h5ad文件的类

位置: runner/src/datamodules/distribution_datamodule.py

类名: TrajectoryNetDistributionTrajectoryDataModule, CustomTrajectoryDataModule, CustomGeodesicTrajectoryDataModule

位置: runner/src/datamodules/components/tnet_dataset.py

类名: CustomAnnDataFromFile, CustomAnnData, SCData.factory

# 12.13
模型模块, 默认配置cfm.yaml

其中关键的配置是流匹配的类: src.models.cfm_module.CFMLitModule

训练向量场的类(即网络模型):src.models.components.simple_mlp.VelocityNet

解微分方程的类:src.models.components.solver.FlowSolver

这些对应文件中都有其他类可以选择, 做实验时尝试不同的类

# 12.14
暂时跳过回调函数callback和日志记录器logger, 与主要实验无关, 是增强作用

## 训练器trainer
pytorch_lightning.Trainer, 封装了训练过程. 主要可以设置最大迭代轮数epoch, 以及设备是cpu还是gpu(默认cpu).

## 训练/测试模块

train.py中默认进行训练和测试. 训练train即拟合网络模型, 记录最佳模型, 返回一个回调矩阵train_metrics. 测试test即使用最佳模型在测试集上评估, 也返回一个回调矩阵test_metrics.

编写脚本1214.py, 在train.py的基础上, 把配置文件改为train_1212.yaml, trainer的设备改为gpu, 然后查看回调矩阵的形状.

trainer的设备改为gpu会报错`Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`原因未知, 解决方法未知, 改回default. 

查看train_metrics和test_metrics.即是模型的主要训练结果.
```powershell
train_metrics：
train/loss: 0.8030401468276978
train/mse: 0.8030401468276978
train/reg: 0.0
val/loss: 1.030808687210083
val/mse: 1.030808687210083
val/reg: 0.0
val/L1: 0.4810240566730499
val/L2: 0.5236572027206421
val/squared_L2: 1.0556195974349976
val/t1/1-Wasserstein: 0.38782596588134766
val/t1/2-Wasserstein: 0.49716320633888245
val/t1/Linear_MMD: 0.03279142081737518
val/t1/Poly_MMD: 0.1810840219259262
val/t1/RBF_MMD: 0.1758289933204651
val/t1/Mean_MSE: 0.04030532017350197
val/t1/Mean_L2: 0.2007618546485901
val/t1/Mean_L1: 0.18626412749290466
val/t1/Median_MSE: 0.5190341472625732
val/t1/Median_L2: 0.655465841293335
val/t1/Median_L1: 0.050951145589351654
val/t2/1-Wasserstein: 0.22572360932826996
val/t2/2-Wasserstein: 0.16165071725845337
val/t2/Linear_MMD: 0.09686477482318878
val/t2/Poly_MMD: 0.3112310767173767
val/t2/RBF_MMD: 0.29641592502593994
val/t2/Mean_MSE: 0.4646153748035431
val/t2/Mean_L2: 0.6067425012588501
val/t2/Mean_L1: 0.03753206133842468
val/t2/Median_MSE: 0.19373193383216858
val/t2/Median_L2: 0.18900945782661438
val/t2/Median_L1: 0.02502826787531376
val/t3/1-Wasserstein: 0.15820324420928955
val/t3/2-Wasserstein: 0.15104889869689941
val/t3/Linear_MMD: 0.5133110284805298
val/t3/Poly_MMD: 0.5813398957252502
val/t3/RBF_MMD: 0.046393267810344696
val/t3/Mean_MSE: 0.21539096534252167
val/t3/Mean_L2: 0.1618097871541977
val/t3/Mean_L1: 0.11369019746780396
val/t3/Median_MSE: 0.3371797800064087
val/t3/Median_L2: 0.3283453583717346
val/t3/Median_L1: 0.4711966305403047
val/t4/1-Wasserstein: 0.5851778739143998
val/t4/2-Wasserstein: 0.041916973888874054
val/t4/Linear_MMD: 0.20398262924517785
val/t4/Poly_MMD: 0.17207473888993263
val/t4/RBF_MMD: 0.06897214008495212
val/t4/Mean_MSE: 0.251843982917935
val/t4/Mean_L2: 0.24051857739686966
val/nfe: 400.0
test_metrics：
test/L1: 0.49579551815986633
test/L2: 0.5496246218681335
test/squared_L2: 1.0491106510162354
test/t1/1-Wasserstein: 0.37634775042533875
test/t1/2-Wasserstein: 0.4966620206832886
test/t1/Mean_MSE: 0.016482973471283913
test/t1/Mean_L2: 0.12838603556156158
test/t1/Mean_L1: 0.12812164425849915
test/t1/Median_MSE: 0.0016637524822726846
test/t1/Median_L2: 0.04078912362456322
test/t1/Median_L1: 0.02913340926170349
test/t2/1-Wasserstein: 0.6395386457443237
test/t2/2-Wasserstein: 0.717170774936676
test/t2/Mean_MSE: 0.18283210694789886
test/t2/Mean_L2: 0.42758870124816895
test/t2/Mean_L1: 0.4112173318862915
test/t2/Median_MSE: 0.3154948949813843
test/t2/Median_L2: 0.5616893172264099
test/t2/Median_L1: 0.5477715730667114
test/t3/1-Wasserstein: 0.3081180155277252
test/t3/2-Wasserstein: 0.37618210911750793
test/t3/Mean_MSE: 0.02705688402056694
test/t3/Mean_L2: 0.1644897758960724
test/t3/Mean_L1: 0.14015881717205048
test/t3/Median_MSE: 0.030091920867562294
test/t3/Median_L2: 0.17347022891044617
test/t3/Median_L1: 0.12875908613204956
test/1-Wasserstein: 0.44133479959952354
test/2-Wasserstein: 0.5300049536370127
test/Mean_MSE: 0.07545732147991657
test/Mean_L2: 0.24015483804009485
test/Mean_L1: 0.22649926443894705
test/Median_MSE: 0.11575018944373976
test/Median_L2: 0.25864955835833475
test/Median_L1: 0.23522135615348816
test/nfe: 400.0
```

# 12.15
尝试能否一次配置多个流匹配的类, 即自动做多个实验
train.py文件中一次只能配置一个数据模块, 一个模型模块, 多个回调函数callback和多个日志记录器logger(因为前两者实例化是单个对象, 后两者实例化是列表), 所以考虑写一个批处理文件, 一次配置多个参数

`scripts/`已经有一些脚本文件, 可供参考

.sh是shell脚本文件, Linux 原生支持 Shell 解释器. windows下可以在安装git bash后使用.sh脚本, 否则可以改写为.bat或者.cmd脚本

用ai改写了`scripts/schedule.sh`, 错误很多, 因为他们很蠢地加了很多中文注释, 导致各种乱码, 之后删除所有中文注释

# 12.16
脚本文件`script/batch_experiments.bat`, 用于一次运行多个模型, 文件中已经标明了使用的模型

使用方法, 注意环境和目录.

```powershell
(cfm-runner) PS D:\desktop\code\conditional-flow-matching\runner> .\scripts\batch_experiments.bat
```

待解决的问题: 

1. 如何配置使用gpu, 前几天配置出错.

2. 默认epoch数为10, 需要调大. 

3. 弹出phate图像会阻塞进程, 应该不输出phate图像.

4. 6.SF2MLitModule...处报错.

解决了gpu的配置:修改runner\src\models\cfm_module.py的preprocess_batch方法(在多个类中都有), 原代码没有正确设置t_select这个变量的设备
```python
t_select = torch.randint(times - 1, size=(batch_size,))    # 修改前
t_select = torch.randint(times - 1, size=(batch_size,), device=X.device)    # 修改后
```

# 12.17
epoch数在`configs/trainer/default.max_epochs`中设置, 调试时暂时设为10, 解决完其他问题, 实验时设置为10000

图像问题, `runner\src\models\components\plotting.store_trajectories`会弹窗绘制散点图, 注释掉这一行:
```python
# scprep.plot.scatter2d(data, c=labels)
```
经过以上处理, 除了sbcfm, sf2m以外的4个模型都可以跑通

批处理文件写得太粗糙了, 应该对每个模型配置不同参数.

待解决的问题:

1. sbcfm, 需要断言数据模块的is_trajectory属性为False, 但是实际是True. 可能是因为这个模型只能处理静态数据, 不适用于时间序列(即轨迹)数据.

2. SF2MLitModule, 需要两个深度学习网络, net和score_net, 不同于其他cfm只需要一个net.  写了一个模型配置文件`configs/model/sf2m.yaml`, 配置和net一样参数的score_net. 还没解决, 需要了解一下flowsolver怎么解微分方程. 

3. 默认配置的模型还没有使用ot采样

# 12.18
论文中没有提到sbcfm不能用于时间序列数据, 暂时注释掉断言, 后续再看是否需要修改.

sf2m模型, 昨天和今天报错不一样? 变成了设备不一致的问题, 前几天解决过类似问题, 用同样的方法修改`runner\src\models\components\simple_mlp.VelocityNet.forward`方法, 把t转移到x的设备上.

查看flowsolver的代码, 没有明显问题, 昨天报错不能复现. 这个类的作用主要是解ode/sde, 返回一个时间序列上的轨迹. 

修改1214.py, 不再手动打印train_metrics和test_metrics, 因为和日志记录的test metric重复了, 需要的可以在日志中看到

配置ot采样, 有四个可选的方法"exact", "sinkhorn", "unbalanced", "partial", 目前使用exact

使用方法, 在运行时设置, 暂时不修改配置文件
```powershell
python "src/1214.py" "model._target_=src.models.cfm_module.CFMLitModule" "model.ot_sampler=exact"
```

最后调整一下类胚体的嵌入embedding类型, 原文用了pca, 代码默认是phate, 调整一下. 如有必要, 之后可以加入tsne的实验

```powershell
python "src/1214.py" "model._target_=src.models.cfm_module.CFMLitModule" "model.ot_sampler=exact" "datamodule.system_kwargs.embedding_name=pca"
```

至此, 将在类胚体数据上进行完整实验. 根据原论文, 主要包含phate以及pca降维的数据, phate是2维, pca是50维. 根据训练难易程度, 先进行phate数据的实验. 新增的入口和配置文件在以下目录:

- `runner/src/train_eb_phate.py`
- `configs/trian_eb_pahte.yaml`
- `configs/datamodule/eb_phate.yaml`
- `configs/trainer/gpu_exp.yaml`
- `scripts/batch_experiments_eb_phate.bat`

运行
`./scripts/batch_experiments_eb_phate.bat`

# 12.19
查看运行结果, 原本输出在`logs/train/runs`目录下, 为了便于后续查看, 复制到`logs/eb_phate/`目录下.

配置并运行pca数据, 之后开始写论文中类胚体数据的实验.

虽然猜测原则上应该在`configs/experiment`下修改默认配置, 重新写train.yaml会导致冗余, 在效果一样的情况下就不计较了. 新增的入口和配置文件在以下目录:

- `src/train_eb_pca.py`
- `configs/train_eb_pca.yaml`
- `configs/datamodule/eb_pca.yaml`
- `scripts/batch_experiments_eb_pca.bat`

输出转移到`logs/eb_pca/`目录下.

# 12.20
原文使用pca数据时, 先将数据截断至前 5 个维度，随后参照 Tong 等人（2020 年）的方法对每个维度进行白化
（whitening）处理，再执行插值操作. 昨天没有进行截断操作, 今天重新进行pca数据的实验, 主要是把`configs/datamodule/eb_pca.yaml`中的`max_dim`设置为5.

今天开始校准参考文献

# 12.21
较对第一章参考文献

# 12.22 
较对第二章参考文献

看日志的时候发现新的问题, ActionMatching和VariancePreservingCFM都没有跑成功, 继续看代码

## ActionMatchingFlow
首先, 也需要断言不是轨迹, 但是数据默认是. 所以跟sbcfm的处理方法一样, 注释掉断言.

需要需要深度学习网络有energy函数, 但是默认使用的VelocityNet没有实现energy函数.尝试唯一有energy函数的类DivergenceFreeNet
```powershell
python "src/train_eb_phate.py" "model._target_=src.models.cfm_module.ActionMatchingLitModule" "trainer.max_epochs=10" "model.net._target_=src.models.components.simple_mlp.DivergenceFreeNet"

```
也不行

考虑重新写一个神经网络.

## VariancePreservingCFM
配置写错类名了, 以及也要注释掉断言



# 12.23
为ActionMatchingFlow增加EnergyVelocityNet, 实现双头设计，既提供了速度场输出用于常规流匹配，又提供了能量场输出用于ActionMatching

```powershell
python "src/train_eb_phate.py" "model._target_=src.models.cfm_module.ActionMatchingLitModule" "trainer.max_epochs=10" "model.net._target_=src.models.components.simple_mlp.EnergyVelocityNet"
```

# 12.24
较对第二章参考文献

# 12.25
符号和缩略语说明