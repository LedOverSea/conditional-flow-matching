

# 通过分数和流匹配实现无模拟的薛定谔桥

## 摘要

我们提出了 *无模拟分数和流匹配* ($$[\mathrm{SF}]^2\mathrm{M}$$)，这是一种无模拟的目标，用于在给定从任意源分布和目标分布中抽取的未配对样本的情况下推断随机动力学。我们的方法推广了用于扩散模型训练的分数匹配损失和最近提出的用于训练连续标准化流的流匹配损失。$$[\mathrm{SF}]^2\mathrm{M}$$ 将连续时间随机生成建模解释为一个薛定谔桥问题。它依赖于静态熵正则化最优传输，或小批量近似，来高效地学习薛定谔桥，而无需模拟学习到的随机过程。我们发现 $$[\mathrm{SF}]^2\mathrm{M}$$ 比先前工作中基于模拟的方法更高效，并且能为薛定谔桥问题提供更精确的解。最后，我们将 $$[\mathrm{SF}]^2\mathrm{M}$$ 应用于从快照数据中学习细胞动力学的问题。值得注意的是，$$[\mathrm{SF}]^2\mathrm{M}$$ 是第一个能够准确建模高维细胞动力学的方法，并且可以从模拟数据中恢复已知的基因调控网络。我们的代码可在 TorchCFM 包中找到：https://github.com/atong01/conditional-flow-matching。

## 1 引言

基于分数的生成模型（SBGMs），包括扩散模型，是一类强大的生成模型，可以表示高维空间上的复杂分布。SBGMs 通常通过模拟源密度（几乎总是高斯分布）根据随机微分方程（SDE）的演化来生成样本。尽管取得了经验上的成功，但 SBGMs 受限于它们对高斯源的假设，这对于使用无模拟去噪目标进行优化至关重要。这个假设在物理或生物系统的时间演化中经常被违反，例如在单细胞基因表达数据的情况下，这阻止了使用 SBGMs 来学习底层动力学。

在此类问题中，首选的方法是使用基于流的生成模型，等同于连续标准化流（CNFs）。基于流的模型假设一个 *确定性* 的连续时间生成过程，并拟合一个将源密度转换为目标密度的常微分方程（ODE）。基于流的模型以前受到低效的基于模拟的训练目标的限制，这些目标需要在训练时对 ODE 进行昂贵的积分。然而，最近的工作引入了无模拟的训练目标，使得当假设高斯源时，CNFs 能够与 SBGMs 竞争，并将这些目标扩展到任意源分布的情况。然而，这些目标尚未适用于学习随机动力学，这对生成建模和恢复系统动力学都是有益的。

薛定谔桥（SB）问题——随机映射两个任意分布的标准概率公式——考虑了在给定参考过程下，源概率分布和目标概率分布之间最可能的演化。SB 问题已被应用于各种各样的问题，包括生成建模、建模自然随机动力系统和平均场博弈。除了少数特殊情况（例如高斯分布），SB 问题通常没有闭式解，但可以通过需要模拟学习到的随机过程的迭代算法来近似。虽然理论上合理，但这些方法存在数值和实际问题，限制了它们向高维度的可扩展性。

本文为薛定谔桥问题引入了一个名为无模拟分数和流匹配 ($$[\mathrm{SF}]^2\mathrm{M}$$) 的无模拟目标。$$[\mathrm{SF}]^2\mathrm{M}$$ 同时推广了 (1) CNFs 的无模拟目标到随机动力学的情况，以及 (2) 扩散模型的去噪训练目标到任意源分布的情况（图 1）。我们的算法利用 SB 问题和熵正则化最优传输（OT）之间的联系，将薛定谔桥表示为布朗桥的混合。与需要在每次迭代中模拟 SDE 的动态 SB 算法相比，$$[\mathrm{SF}]^2\mathrm{M}$$ 利用了源分布和目标分布之间的静态熵正则化 OT 映射，这些映射可以通过 Sinkhorn 算法高效计算。

我们在合成和真实世界数据集上证明了 $$[\mathrm{SF}]^2\mathrm{M}$$ 的有效性。在合成数据上，我们表明 $$[\mathrm{SF}]^2\mathrm{M}$$ 比相关的先前工作表现更好，并且找到了对真实薛定谔桥的更好近似。

作为对真实数据的应用，我们考虑通过一系列薛定谔桥来建模横截面测量序列（即未配对的时间序列观测）。虽然有许多先前的方法在静态设置或低维动态设置下用薛定谔桥建模细胞，但 $$[\mathrm{SF}]^2\mathrm{M}$$ 是第一个能够扩展到数千个基因维度的方法，因为它的训练是完全无模拟的。我们还引入了一个静态流形测地线映射，它改善了动态设置下的细胞插值，展示了具有非欧几里得成本的薛定谔桥近似的首批实际应用之一。最后，我们表明，与静态最优传输不同，我们能够直接建模和恢复驱动细胞动力学的基因-基因相互作用网络。

我们总结了我们的 **主要贡献** 如下：

*   我们提出了 $$[\mathrm{SF}]^{2}\mathrm{M}$$，这是薛定谔桥问题的第一个无模拟目标，并证明了其正确性。
*   我们研究了用于 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 的熵正则化 OT 计划的有效经验和最小批量近似。
*   我们在合成分布和几个单细胞动力学问题上验证了我们提出的方法。

## 2 预备知识

我们考虑一对在 $$\mathbb{R}^{d}$$ 上的紧支撑分布，具有（未知的）密度 $$q(x_{0})$$ 和 $$q(x_{1})$$（也记为 $$q_{0},q_{1}$$）。我们假设可以访问来自 $$q_{0}$$ 和 $$q_{1}$$ 的有限样本数据集。连续时间随机生成建模或 SDE 推断的问题在于找到一个将 $$q_{0}$$ 转换为 $$q_{1}$$ 的随机映射 $$f$$。然后可以通过从 $$q_{0}$$ 中抽取样本并对该样本应用 $$f$$ 以获得来自 $$q_{1}$$ 的样本来生成来自 $$q_{1}$$ 的样本。

### 2.1 通过熵正则化 OT 实现薛定谔桥

薛定谔桥问题要求找到两个概率测度 $$q_{0}$$ 和 $$q_{1}$$ 之间相对于参考随机过程 $$\mathbb{Q}$$ 的最可能演化。形式上，薛定谔桥是以下问题的解：

$$
\mathbb{P}^{*}=\operatorname*{arg\,min}_{\mathbb{P}:p_{0}=q_{0},p_{1}=q_{1}} \mathrm{KL}(\mathbb{P} \parallel \mathbb{Q}),
$$

其中 $$\mathbb{P}$$ 是一个随机过程（在连续路径 $$[0,1]\rightarrow\mathbb{R}^{d}$$ 上的分布），其定律为 $$p$$（边缘分布记为 $$p_{t}$$）。

**SDEs 和扩散过程。** 我们考虑的随机过程可以表示为形式为 $$dx=u_{t}(x)\,dt+g(t)\,dw_{t}$$ 的 Ito SDE，其中 $$u_{t}$$ 是一个平滑向量场，$$dw_{t}$$ 是布朗运动。根据 SDE 演化的密度 $$p(x_{0})$$ 诱导了边缘分布 $$p_{t}(x_{t})$$，被视为函数 $$p:[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}_{+}$$。它们由初始条件 $$p_{0}$$ 和 *福克-普朗克方程* $$\partial_{t}p_{t}=-\nabla\cdot(p_{t}u_{t})+\frac{g^{2}(t)}{2}\Delta p_{t}$$ 表征，其中 $$\Delta p_{t}=\nabla\cdot(\nabla p_{t})$$ 是拉普拉斯算子。

在这项工作中，我们考虑 $$\mathbb{Q}=\sigma\mathbb{W}$$，其中 $$\mathbb{W}$$ 是由 SDE $$dx=dw_{t}$$ 定义的标准布朗运动，在这种情况下，(1) 的解被称为*扩散薛定谔桥*。我们参考 Leonard 对薛定谔桥的完整讨论。

**熵正则化最优传输。** 熵正则化 OT 问题定义如下：
$$
\pi^{*}_{\varepsilon}(q_{0},q_{1})= \operatorname*{arg\,min}_{\pi\in U(q_{0},q_{1})}\int d(x_{0},x_{1 })^{2}\,d\pi(x_{0},x_{1})+\varepsilon\,\mathrm{KL}(\pi\|q_{0}\otimes q_{1}),
$$

其中 $$U(q_{0},q_{1})$$ 是允许的传输计划集合（$$x_{0}$$ 和 $$x_{1}$$ 上的联合分布，其边缘分布等于 $$q_{0}$$ 和 $$q_{1}$$），$$d(\cdot,\cdot)$$ 是基础成本，$$\varepsilon$$ 是正则化参数，$$q_{0}\otimes q_{1}$$ 是 $$x_{0},x_{1}$$ 上的联合分布，其中 $$x_{0}$$ 和 $$x_{1}$$ 是独立的。当 $$\varepsilon\to 0$$ 时，我们恢复*精确最优传输*。我们现在回顾一个连接 SB 问题与熵正则化 OT 计划的基石定理：

**命题 2.1** (Follmer, 1988)。令参考过程为布朗运动（即 $$\mathbb{Q}=\sigma\mathbb{W}$$）。那么薛定谔桥问题承认一个唯一的解 $$\mathbb{P}^{*}$$，其形式为由熵正则化 OT 计划加权的布朗桥的混合：
$$
\mathbb{P}^{*}((x_{t})_{t\in[0,1]})\!=\!\int\mathbb{Q}((x_{t})_{t} \mid x_{0},x_{1})\,d\pi^{*}_{2\sigma^{2}}(x_{0},x_{1})
$$

*其中 $$\mathbb{Q}((x_{t})_{t\in(0,1)} \mid x_{0},x_{1})$$ 是 $$x_{0}$$ 和 $$x_{1}$$ 之间具有扩散率 $$\sigma$$ 的布朗桥。*

受此结果启发，我们提出的算法随机地将定义无条件 SDE 的参数回归到定义布朗桥的参数。

### 2.2 神经 SDE 和概率流

在本节中，我们考虑一个 SDE $$dx=u_{t}(x)\,dt+g(t)\,dw_{t}$$。我们回顾一些重要的性质，并讨论通过神经网络近似 $$u_{t}$$。

**分数和流参数化。** 在退化情况 $$g(t)\equiv 0$$ 下，SDE 变为 ODE，并且福克-普朗克方程恢复*连续性方程* $$\frac{\partial p}{\partial t}=-\nabla\cdot(p_{t}u_{t})$$。从福克-普朗克方程和连续性方程可以很容易地推导出 ODE
$$
dx=\underbrace{\left[u_{t}(x)-\frac{g(t)^{2}}{2}\nabla\log p_{t}(x)\right]}_{u^{o}_{t}(x)}dt,
$$

连同初始条件 $$p(x_{0})$$ 上的分布，诱导了与 SDE 相同的边缘分布 $$p_{t}(\cdot)$$；因此，(4) 被称为随机过程的*概率流 ODE*。相反，如果概率流 ODE 的漂移 $$u^{o}_{t}(x)$$、扩散调度 $$g(\cdot)$$ 和*分数函数* $$\nabla\log p_{t}(x)$$ 已知，则 SDE 的漂移项可以通过下式恢复

$$
u_{t}(x)=u^{o}_{t}(x)+\frac{g(t)^{2}}{2}\nabla\log p_{t}(x).
$$

因此，**指定一个 SDE 等同于指定概率流 ODE 及其分数函数。** 通过在 ODE (4) 中反转 $$u_{t}^{o}$$ 的符号并使用 (5) 将其转换为 SDE，我们也得到了来自 Anderson (1982) 的时间反转公式：

$$
dx =\left[-u_{t}^{o}(x)+\frac{g(t)^{2}}{2}\nabla\log p_{t}(x)\right]dt+ g(t)\,dw_{t} =\left[-u_{t}(x)+g(t)^{2}\nabla\log p_{t}(x)\right]dt+g(t)\,dw_{t},
$$

它在 $$x_{1-t}$$ 上诱导的分布与原始 SDE 在 $$x_{t}$$ 上诱导的分布相同。

**用神经网络近似 SDE。** 如果边缘分布 $$p_{t}(x)$$ 可以被容易地采样，并且知道概率流 ODE 的漂移 $$u_{t}^{o}(x)$$ 和分数 $$\nabla\log p_{t}(x)$$，则两者都可以通过神经网络来近似。具体来说，时变向量场 $$v_{\theta}(\cdot,\cdot):[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$$ 和 $$s_{\theta}(\cdot,\cdot):[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}_{d}$$ 可以通过*（无条件）分数和流匹配* 目标进行训练

$$
\mathcal{L}_{\text{U}[\text{SF}]^{2}\text{M}}(\theta)= \mathbb{E}\big{[}\|v_{\theta}(t,x)-u_{t}^{o}(x)\|^{2}+ \lambda(t)^{2}\, \|s_{\theta}(t,x)-\nabla\log p_{t}(x)\|^{2}\big{]},
$$

其中期望是关于 $$t\sim\mathcal{U}(0,1)$$, $$x\sim p_{t}(x)$$ 的，并且 $$\lambda(\cdot)$$ 是某个正权重的选择。（在实践中，近似 $$g(t)^{2}\nabla\log p_{t}(x)$$ 可能比近似 $$\nabla\log p_{t}(x)$$ 更稳定，这是一个简单的参数化更改，不会改变学习问题或目标。）一旦训练完成，$$v_{\theta}$$ 和 $$s_{\theta}$$ 可以用于从源样本 $$x_{0}$$ 模拟 SDE。此过程在算法 2 中描述。

值得注意的是，通过概率流 ODE 和分数的单独参数化，我们可以在推理时使用任意的扩散率 $$g(\cdot)$$ 来模拟 SDE，该扩散率不需要与训练时使用的扩散率匹配。如果达到了 (7) 的全局最优（图 1），我们确保对于任何任意的扩散率 $$g(\cdot)$$ 都能获得相同的边缘分布样本。例如，我们通过设置 $$g(t)\equiv 0$$ 来模拟概率流 ODE。类似地，可以使用时间反转公式 (6) 从样本 $$x_{1}$$ 开始模拟反向 SDE。

**布朗桥的 ODEs 和 SDEs。** 对于边缘分布 $$p_{t}(x)$$ 是高斯的 随机过程，Lipman 等人的定理 3 或 Tong 等人的定理 2.1 给出了流和分数的表达式。主要感兴趣的情况是从 $$x_{0}$$ 到 $$x_{1}$$ 的布朗桥，具有恒定扩散率 $$g(t)=\sigma$$。边缘分布由 $$p_{t}(x)=\mathcal{N}(x;tx_{1}+(1-t)x_{0},\sigma^{2}t(1-t))$$ 给出，并且使用上述结果计算 ODE 和分数：
$$
u_{t}^{o}(x) =\frac{1-2t}{t(1-t)}(x-(tx_{1}+(1-t)x_{0}))+(x_{1}-x_{0}), \\ \nabla\log p_{t}(x) =\frac{tx_{1}+(1-t)x_{0}-x}{\sigma^{2}t(1-t)}.
$$

我们将提出的薛定谔桥近似算法利用了熵正则化 OT 问题 (2) 的快速解和布朗桥的闭式 $$u_{t}^{o}$$ 和 $$\nabla\log p_{t}$$ (8)。

## 3 无模拟 SDE 训练

接下来我们描述我们通过分数和流匹配来学习 SDE 的无模拟方法，总结在算法 1 中。我们在 §3.1 中介绍一般情况，然后在 §3.2 中考虑薛定谔桥的情况。

### 3.1 匹配条件流和分数

Tong 等人描述了一种无模拟的随机回归目标，条件流匹配（CFM），它拟合一个生成由更简单概率路径的混合给出的边缘分布的 ODE。我们将 CFM 推广到匹配随机动力学。

假设随机过程 $$\mathbb{P}((x_{t})_{t\in[0,1]})$$，其边缘分布为 $$p_{t}(x)$$，是关于具有密度 $$q(z)$$ 的潜变量 $$z$$ 的混合，*即*，

$$
\mathbb{P}((x_{t})_{t\in[0,1]})=\int\mathbb{P}((x_{t})|z)q(z)\,dz.
$$

假设 $$\mathbb{P}((x_{t})|z)$$ 由 SDE $$dx=u_{t}(x|z)\,dt+g(t)\,dw_{t}$$ 定义，具有初始条件 $$p_{0}(x|z)$$，并令 $$u_{t}^{o}(x|z)$$ 为由 (4) 给出的相应概率流 ODE 的漂移。然后，对于生成过程 $$\mathbb{P}$$ 给定初始条件 $$p_{0}(x)=\int_{z}p_{0}(x|z)q(z)\,dz$$ 的边缘分布，可以得到概率流 ODE 和分数的表达式：

$$
u_{t}^{o}(x) =\mathbb{E}_{q(z)}\frac{u_{t}^{o}(x|z)p_{t}(x|z)}{p_{t}(x)}, \\ \nabla\log p_{t}(x) =\mathbb{E}_{q(z)}\left[\frac{p_{t}(x|z)}{p_{t}(x)}\nabla\log p_{t }(x|z)\right].
$$

准确地说，我们将 Tong 等人的定理 3.1 推广到随机设置：

**定理 3.1。** *在温和的正则性条件下，ODE $$dx=u_{t}^{o}(x)\,dt$$ 从初始条件 $$p_{0}$$ 生成 $$\mathbb{P}$$ 的边缘分布 $$p_{t}$$，并且分数由 (10) 给出。SDE $$dx=[u_{t}^{o}(x)+\frac{1}{2}g(t)^{2}\nabla\log p_{t}(x)]\,dx+g(t)\,dt$$ 生成 $$\mathbb{P}$$ 的马尔可夫化过程。*

一般来说，定理 3.1 中的 SDE 不能恢复 $$\mathbb{P}$$，只能恢复它的马尔可夫化过程（*即*，具有相同无穷小转移核的过程）。形式为 (9) 的过程 $$\mathbb{P}$$ 不一定是马尔可夫的，并且可能不由任何 SDE 生成。

**一个随机回归目标。** (10) 中的边缘 ODE 漂移和分数表达式激发了用神经网络拟合 $$u_{t}^{o}(x)$$ 和 $$\nabla\log p_{t}(x)$$ 的目标，当只有条件 ODE 和分数已知时。

推广 (7)，我们定义了（条件）无模拟分数和流匹配目标（$$[\mathrm{SF}]^{2}\mathrm{M}$$），用于神经网络 $$v_{\theta}(\cdot, \cdot)$$ 近似 ODE 漂移和 $$s_{\theta}(\cdot, \cdot)$$ 近似分数：

$$
\mathcal{L}_{[\text{SF}]^{2}\text{M}}(\theta) = \mathbb{E}_{Q'} \left[ \|v_{\theta}(t, x) - u_{t}^{o}(x|z)\|^{2} \right] \\ + \mathbb{E}_{Q'} \left[ \lambda(t)^{2} \|s_{\theta}(t, x) - \nabla\log p_{t}(x|z)\|^{2} \right],
$$

其中，与 (7) 中一样，$$\lambda(\cdot)$$ 是某个正权重的选择，并且 $$Q' = (t \sim \mathcal{U}(0, 1)) \otimes q(z) \otimes p_{t}(x|z)$$。只要条件 ODE 和分数已知并且 $$p_{t}(x|z)$$ 可以被容易地采样，这个目标就可以用来近似 (10) 中定义的量。正确性由以下定理保证：

**定理 3.2（条件梯度相等）。** *如果对于所有 $$x \in \mathbb{R}^{d}$$ 和 $$t \in [0, 1]$$ 有 $$p_{t}(x) > 0$$，那么 $$\nabla_{\theta}\mathcal{L}_{\text{U}[\text{SF}]^{2}\text{M}}(\theta) = \nabla_{\theta}\mathcal{L}_{[\text{SF}]^{2}\text{M}}(\theta)$$，其中 $$\mathcal{L}_{\text{U}[\text{SF}]^{2}\text{M}}(\theta)$$ 是无条件分数和流匹配损失 (7)。*

这个结果推广了 Tong 等人的定理 3.2。它提供了一种无模拟的方式来训练神经网络，足以模拟生成边缘分布 $$p_{t}(x)$$ 且具有任意扩散率 $$g(\cdot)$$ 的 SDE（参见 (7) 之后的讨论）。训练和推理算法总结在算法 1 和算法 2 中。

在我们的方法中，通过 (10) 定义的 ODE 和分数恢复的 SDE 是关于 $$z$$ 索引的随机过程混合的马尔可夫化。

**条件 ODE 和分数的来源。** 尽管 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 框架可以处理一般的条件信息 $$z$$，但在本文中，我们考虑 $$z$$ 被识别为源点和目标点对 $$(x_{0}, x_{1})$$ 的情况。对于给定的 $$z = (x_{0}, x_{1})$$，我们将条件概率路径 $$p_{t}(x|z)$$ 设为具有恒定扩散尺度 $$\sigma$$ 的布朗桥，因此 $$u_{t}^{o}(x|z)$$ 和 $$\nabla\log p_{t}(x|z)$$ 由 (8) 给出。为了避免在 $$t$$ 接近 0 或 1 时出现数值问题，我们在方差上添加了一个小的平滑常数。因此，条件分布在 $$t=0$$ 和 $$t=1$$ 时在 $$x_{0}$$ 和 $$x_{1}$$ 处是尖峰的。（附录 E 中描述了非恒定扩散尺度的扩展。）

为了使得到的边缘分布 $$p_{t}(x)$$ 满足边界条件 $$p_{0}(x) = q_{0}(x)$$ 和 $$p_{1}(x) = q_{1}(x)$$，$$q(x_{0}, x_{1})$$ 必须是 $$q_{0}$$ 和 $$q_{1}$$ 的耦合（即，一个传输计划）。这在以下定理中形式化：

**定理 3.3（$$[\mathrm{SF}]^{2}\mathrm{M}$$ 从桥恢复边缘分布）。** 如果 $$q(\cdot, \cdot) \in U(q_{0}, q_{1})$$ 并且 $$v^{*}_{\theta}$$, $$s^{*}_{\theta}$$ 全局最小化 $$\mathcal{L}_{[\text{SF}]^{2}\text{M}}(\theta)$$，那么具有漂移 $$[v^{*}_{\theta} + \frac{1}{2}g(t)^{2}s^{*}_{\theta}]$$、扩散 $$g$$ 和初始条件 $$p_{0} = q_{0}$$ 的 SDE 是关于 $$q(x_{0}, x_{1})$$ 的布朗桥混合的马尔可夫过程。特别地，如果 SDE 生成边缘分布 $$p_{t}$$，那么 $$p_{1} = q_{1}$$。

这个定理告诉我们，只要我们的联合分布 $$q(x_{0}, x_{1})$$ 具有正确的边缘分布，$$[\mathrm{SF}]^{2}\mathrm{M}$$ 将恢复一个有效的生成模型，该模型将 $$q_{0}$$ 推送到 $$q_{1}$$。

### 3.2 通过 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 和熵最优传输构建薛定谔桥

在上一节中，我们展示了我们的方法 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 可以近似形式为 (9) 的过程混合的边缘概率 $$p_{t}$$。在本节中，我们解释我们的 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 如何近似 SB。

**$$[\mathrm{SF}]^{2}\mathrm{M}$$ 近似薛定谔桥。** 为了实现 SB 的高效近似，我们利用了命题 2.1。SB 可以表示为由熵正则化最优传输计划加权的布朗桥的混合 (3)。因此，为了用 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 近似 SB，我们将分布 $$q(x_{0}, x_{1})$$ 设置为等于熵正则化 OT 计划 $$\pi^{*}_{2\sigma^{2}}(q_{0}, q_{1})$$，并使用算法 1 训练网络 $$v_{\theta}$$ 和 $$s_{\theta}$$。我们证明这个过程恢复了 SB：

**命题 3.4（使用熵正则化 OT 的 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 恢复 SB 过程）。** 令 $$\mathbb{P}^{*}$$ 为关于 $$\mathbb{Q}=\sigma\mathbb{W}$$ 的 $$q_{0}$$ 和 $$q_{1}$$ 之间的薛定谔桥。如果 $$v^{*}_{\theta}$$, $$s^{*}_{\theta}$$ 全局最小化 $$\mathcal{L}_{[\text{SF}]^{2}\text{M}}$$，且耦合为 $$\pi^{*}_{2\sigma^{2}}(q_{0}, q_{1})$$，那么 $$\mathbb{P}^{*}$$ 由具有漂移 $$[v^{*}_{\theta} + \frac{1}{2}g(t)^{2}s^{*}_{\theta}]$$、扩散 $$g$$ 和初始条件 $$p_{0} = q_{0}$$ 的 SDE 定义。

**经验近似。** 不幸的是，真实分布 $$q_{0}$$ 和 $$q_{1}$$ 通常是未知的，我们只能访问形成大小为 $$n$$ 的经验分布 $$\hat{q}_{0}$$ 和 $$\hat{q}_{1}$$ 的独立同分布样本。因此，我们只能通过计算经验分布之间的熵正则化 OT 计划 $$\pi^{*}_{2\sigma^{2}}(\hat{q}_{0}, \hat{q}_{1})$$ 来近似真实的熵正则化 OT 计划。这个经验 OT 计划可以在 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 中用于构建一个经验薛定谔桥。

幸运的是，即使在高维空间中，真实的熵正则化 OT 也可以使用经验分布高效地近似，并且最近表明薛定谔桥继承了这一性质。反过来，经验分布之间的熵正则化 OT 计划可以使用 Sinkhorn 算法高效计算，该算法具有 $$\mathcal{O}(n^{2})$$ 的计算复杂度，或者使用随机算法。然而，如果这个成本太高（*例如*，如果 $$n$$ 太大，或者如果存在真实的生成过程，如在高斯到数据设置中），该计划可以使用小批量 OT 进一步近似；参见附录 A。

使用熵正则化 OT 计划并通过随机回归进行边缘化，将 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 与现有的神经 SB 算法区分开来（表 1）。这类过去的方法包括均值匹配（DSB 和 NLSB）和桥匹配方法（DSBM 和 IDBM），两者都需要一个外部的迭代比例拟合循环和一个内部的训练循环。其他人研究了假设有配对的源和目标数据的问题（I²SB 和 ASB）；SF²M 可以被认为是*推断*配对的同时联合拟合 SDE。

关于这些选择的含义和实践建议的进一步讨论，请参见 §C.3。

## 4 使用 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 学习细胞动力学

建模细胞动力学是单细胞数据科学中的一个主要开放问题，因为它对于理解——并最终干预——发育和疾病的细胞程序非常重要。在本节中，我们展示 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 如何用于并适应于建模单细胞动力学。

时间分辨快照数据之间的细胞动力学，代表位于基因激活空间中的细胞的观测，通常使用薛定谔桥建模。SB 公式对细胞动力学的适用性依赖于最小作用量原理，该原理被认为在短时间尺度上适用于细胞系统，并激励我们选择将 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 应用于这些问题。

**在细胞流形上学习流。** 细胞被认为位于基因表达空间中的低维流形上，这激发了关于密度 adhering 正则化和流形嵌入的工作。因为 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 可以使用由具有任意成本函数的熵正则化 OT 定义的边缘分布 $$q(x_{0}, x_{1})$$ 之间的耦合，我们可以利用这些嵌入来计算适应于流形几何形状的基础成本的配对。具体来说，我们使用 Geodesic Sinkhorn 方法，该方法计算具有成本的熵正则化 OT 计划

$$
c_{\text{geo}}(x_{0},x_{1})=\sqrt{-\log\mathcal{H}_{t}(x_{0},x_{1})}.
$$

矩阵 $$\mathcal{H}_{t}$$ 近似了通过流形上的拉普拉斯-贝尔特拉米算子定义的热核，使用 $$k$$-最近邻图高效近似。我们发现使用这种成本在高维中导致更准确的轨迹（见表 5）。

**学习发育景观。** 细胞发育的一个常见模型，称为 Waddington 的表观遗传景观，假设细胞在基因表达空间中通过遵循能量函数的（有噪声的）梯度上升来进化和分化。虽然之前已经提出了一些启发式方法来从单细胞数据中近似这个能量函数，但我们提出了一种新的方法来直接建模这个景观。在我们的方法中，负能量可以直接解释为动作势，灵感来自 Neklyudov 等人中的建模。

为此，我们在 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 中对流和分数施加了一个朗之万动力学参数化：$$v_{\theta}(t,x)=-\nabla_{x}E_{v}(t,x)$$ 和 $$s_{\theta}(t,x)=-\nabla_{x}E_{s}(t,x)$$，其中 $$E_{v}$$ 和 $$E_{s}$$ 是神经网络。我们可以将 Waddington 的景观定义为 $$W:=E_{v}+\frac{1}{2}g(t)^{2}E_{s}$$。那么 SDE 的漂移是 $$u_{t}(x)=-\nabla_{x}W(t,x)$$，这意味着细胞的时间演化遵循 $$W$$ 上的梯度动力学，并添加了尺度为 $$g(t)$$ 的高斯噪声。我们在图 2 中可视化了这些景观，并在 §F.6 中提供了更多细节。

**学习基因调控网络。** 最后，我们使用 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 从基因表达的群体快照中学习基因调控网络，这是细胞生物学中一个持续存在的挑战。遵循先前从连续时间系统发现稀疏交互结构的工作，我们将基因调控网络定义为有向图，其顶点是基因（空间的维度），并且当且仅当 $$\frac{\partial(v_{\theta}(t,x))_{j}}{\partial x_{i}} \neq 0$$ 时存在边 $$i \rightarrow j$$。这个有向图预计是稀疏的。

先前的工作诉诸于在低维（且密集的）表示中执行轨迹推断，这使发现基因空间中的稀疏图结构复杂化。$$[\mathrm{SF}]^{2}\mathrm{M}$$ 是第一个扩展到高维度的薛定谔桥方法。这使我们能够直接在基因空间中学习动力学并恢复稀疏的基因相互作用。为了实现这一点，我们使用了 $$v_{\theta}$$ 的专门参数化，灵感来自 Bellot and Branson，这使得图结构可以从训练模型的初始层的稀疏模式中读出（详见 §F.7）。

## 5 相关工作

**随机连续时间建模。** 我们的框架与基于流的和基于分数的生成建模都相关。两者都因其在训练中的稳定性和效率以及生成样本的高质量而受到关注。进一步讨论请参见附录 C。

**薛定谔桥近似方法。** 虽然关于 SB 问题有大量的理论工作，但实际的解决方案假设了来自薛定谔桥的配对样本或需要在训练期间进行模拟。基于迭代比例拟合的算法（DSB 和 DSBM）具有如果在每次迭代中都训练到最优则产生精确薛定谔桥的优点，但由于欠拟合和函数近似，可能会在每一步外循环中累积误差。另一方面，我们提出的 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 既不需要训练时积分也不需要外循环迭代，因此如果神经网络函数类和学习算法允许，将收敛到精确的 SB——但与 DSB 和 DSBM 不同，需要知道熵正则化 OT 计划和条件路径（见表 1）。

这些算法的相对优势和实际建议在 §3.2, §C.3 中进一步讨论。此外，在附录 D 中，我们展示了基于模拟的外循环可以与 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 结合，以生成性能为代价改善 SB 边缘分布。

**在细胞动力学中的应用。** 当观察者寻求从具有 scRNA-seq 数据的多个快照中恢复动力学时，可以使用最优传输的机制。然而，这些方法都需要在训练期间进行模拟，这在扩展到高维度时效果不佳。

## 6 实验

# ！！！！不完整，需要重新翻译



在本节中，我们根据最优传输、生成建模和单细胞插值标准对 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 进行实证评估。我们比较：

*   具有精确 OT 小批量的 Minibatch $$[\mathrm{SF}]^{2}\mathrm{M}$$（$$[\mathrm{SF}]^{2}\mathrm{M}$$-Exact），具有熵正则化 OT（Sinkhorn）小批量（-Sink），具有独立耦合（-I），以及在适用时具有测地线 OT（-Geo）。
*   各种（ODE）基于流的模型，包括最优传输条件流匹配（OT-CFM），整流流（RF）和流匹配（FM）。
*   薛定谔桥模型：扩散薛定谔桥（DSB）和扩散薛定谔桥匹配（DSBM），这等同于迭代扩散混合传输的工作（IDBM）。
*   单细胞动力学模型：神经拉格朗日薛定谔桥（NLSB），TrajectoryNet。

所有实验细节请参见附录 F。所有结果均表示为五个种子的平均值 ± 标准差。

**$$[\mathrm{SF}]^{2}\mathrm{M}$$ 是低维数据的有竞争力的生成模型。** 我们首先在表 2 中评估各种方法在低维数据集（8gaussians, moons, scurve）上近似动态最优传输的效果。我们使用 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 训练高斯与每个数据集之间以及 8gaussians 与 moons 之间的薛定谔桥。我们报告了预测分布与目标分布之间的 2-Wasserstein 距离，样本量为 10,000。遵循 Tong 等人，我们还报告了相对于 2-Wasserstein 距离的归一化路径能量，定义为 $$\text{NPE}(p,q):=|\int\|v_{\theta}(t,x)\|^{2}dt-\mathcal{W}_{2}^{2}(p,q)|/\mathcal {W}_{2}^{2}(p,q)$$。当且仅当 $$v_{\theta}$$ 解决了动态最优传输问题时，该度量等于零。表 2 总结了我们的结果，显示 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 优于所有方法，无论是随机的（顶部）还是确定性的（底部）。尽管存在小批量 OT 偏差（可以看作是一种正则化形式，如熵正则化），我们发现 $$[\mathrm{SF}]^{2}\mathrm{M}$$-Exact 最好地近似了薛定谔桥，在批次大小为 512 时，OT 计算仅占训练时间的 1%。

**$$[\mathrm{SF}]^{2}\mathrm{M}$$ 恢复 SB。** 接下来我们评估 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 建模薛定谔桥边缘分布的能力。我们使用高斯到高斯的薛定谔桥，因为它具有闭式高斯边缘分布，遵循 De Bortoli et al.。在所有方法训练之后，我们通过使用算法 2 采样轨迹来评估经验边缘分布相对于地面实况的质量。我们在多个时间点计算经验边缘分布的高斯近似与地面实况薛定谔桥的高斯边缘分布之间的 KL 散度。该评估在表 3 中仅显示在最后一个时间点（$$t=1$$）和 21 个等间距时间点的平均值。我们对每种方法训练相同的步数，对 DSB 和 DSBM 使用 20 个外循环，这需要前向和后向模型的迭代优化。我们发现在低维中，SB-CFM（对应于 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 的概率 ODE 流）表现最好，紧随其后的是 $$[\mathrm{SF}]^{2}\mathrm{M}$$-Exact。在高维中，$$[\mathrm{SF}]^{2}\mathrm{M}$$-Exact 更好地匹配目标分布，并且在中间边缘分布上表现与 DSBM 相似，并且显著优于 DSB。

**$$[\mathrm{SF}]^{2}\mathrm{M}$$ 准确建模高维单细胞动力学。** 我们在单细胞动力学上训练我们的方法 $$[\mathrm{SF}]^{2}\mathrm{M}$$，如 §4 所述，在 Tong 等人建立的设置中的三个真实世界数据集上（见 §F.6），并在表 4 和表 5 中收集了不同维度的结果。给定 $$K$$ 个表示在不同时间点的细胞群体的未配对数据分布，我们解决每两个连续时间点之间的 SB 问题，在模型之间共享参数。为了测试训练模型的插值能力，我们执行留一法插值，使用在除时间点 $$k$$ 之外的所有时间点上训练的模型预测时间点 $$k$$，然后测量预测分布与地面实况分布之间的 1-Wasserstein 距离。我们发现 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 在所有数据集和维度上都优于或与最先进的方法相当。值得注意的是，$$[\mathrm{SF}]^{2}\mathrm{M}$$ 是第一个能够扩展到 1000 个基因维度的方法，并且在使用测地线成本时表现最好。

**$$[\mathrm{SF}]^{2}\mathrm{M}$$ 恢复基因调控网络。** 最后，我们评估 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 从模拟的单细胞基因表达数据中恢复基因调控网络（GRN）的能力。我们使用一个专门的神经图形模型（NGM）参数化，如 §4 所述，在模拟的分叉和三叉系统上训练 $$[\mathrm{SF}]^{2}\mathrm{M}$$。我们测量了恢复的 GRN 与用于生成数据的真实 GRN 之间的 AUC-ROC 和平均精度（AP）。结果在表 6 中显示，表明 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 优于所有基线方法。

## 7 结论

我们引入了一类新的无模拟目标，用于学习在一般源分布和目标分布之间的连续时间随机生成模型。对于具有有限支持的源和目标，我们可以通过高效算法计算熵正则化 OT 计划，直接近似连续时间薛定谔桥而无需模拟。我们已经展示了我们的方法如何应用于学习细胞动力学和提取基因调控结构。未来的工作可以考虑如何使用干预数据训练类似 $$[\mathrm{SF}]^{2}\mathrm{M}$$ 的模型以改进 GRN 推断。

**局限性。** $$[\mathrm{SF}]^{2}\mathrm{M}$$ 的主要限制是它需要知道条件路径分布（布朗桥）。如果考虑更一般的参考过程（这可能有助于编码生物学先验），或者在一般黎曼流形上，这些分布没有闭式形式。

## 致谢

（略）

## 参考文献

（略）

## 清单

（略）

## 通过分数和流匹配实现无模拟的薛定谔桥：补充材料

（略）
